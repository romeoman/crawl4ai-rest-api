---
description:
globs:
alwaysApply: false
---
# AI Provider Configuration Patterns

## **Flexible Provider Format Support**
- **Support multiple naming conventions**:
  - Prefixed format: `"openai/gpt-4o-mini"`, `"anthropic/claude-3-sonnet"`
  - Custom models: `"gpt-4.1-nano-2025-04-14"`
  - Legacy format: `"gpt-4"`, `"claude-3-sonnet"`

## **Provider Detection Logic**
- **Auto-detect provider from model name**:
```python
def detect_provider(provider_or_model: str) -> tuple[str, str]:
    """Return (provider_name, model_name) tuple"""
    if "/" in provider_or_model:
        return provider_or_model.split("/", 1)
    
    # Auto-detect based on model name patterns
    if "claude" in provider_or_model.lower():
        return "anthropic", provider_or_model
    elif "gemini" in provider_or_model.lower():
        return "google", provider_or_model
    elif "gpt" in provider_or_model.lower():
        return "openai", provider_or_model
    else:
        return "openai", provider_or_model  # Default fallback
```

## **Environment Variable Mapping**
- **Standard API key patterns**:
```python
PROVIDER_ENV_MAPPING = {
    "openai": "OPENAI_API_KEY",
    "anthropic": "ANTHROPIC_API_KEY", 
    "google": "GOOGLE_API_KEY",
    "perplexity": "PERPLEXITY_API_KEY",
    "mistral": "MISTRAL_API_KEY",
    "azure": "AZURE_OPENAI_API_KEY"
}

def get_api_key(provider: str, provided_token: Optional[str] = None) -> str:
    if provided_token:
        return provided_token
    
    env_key = PROVIDER_ENV_MAPPING.get(provider)
    if not env_key:
        raise ValueError(f"Unknown provider: {provider}")
    
    api_key = os.getenv(env_key)
    if not api_key:
        raise ValueError(f"API key not found for {provider}. Set {env_key}")
    
    return api_key
```

## **Modern LLMConfig Usage**
- **Use LLMConfig instead of deprecated parameters**:
```python
def create_llm_config(provider: str, model: str, api_key: str) -> LLMConfig:
    """Create LLMConfig with proper provider format"""
    return LLMConfig(
        provider=f"{provider}/{model}",
        api_token=api_key
    )

# ✅ DO: Modern approach
llm_config = create_llm_config("openai", "gpt-4o-mini", api_key)
strategy = LLMExtractionStrategy(
    llm_config=llm_config,
    instruction=instruction
)

# ❌ DON'T: Deprecated approach (causes warnings)
strategy = LLMExtractionStrategy(
    provider="openai",
    api_key=api_key,
    model="gpt-4"
)
```

## **Provider-Specific Configuration**
- **Handle provider-specific requirements**:
```python
def configure_provider_specific(provider: str, config: dict) -> dict:
    """Apply provider-specific configurations"""
    if provider == "azure":
        config["azure_endpoint"] = os.getenv("AZURE_OPENAI_ENDPOINT")
        if not config["azure_endpoint"]:
            raise ValueError("AZURE_OPENAI_ENDPOINT required for Azure provider")
    
    elif provider == "ollama":
        config["base_url"] = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/api")
    
    return config
```

## **Error Handling and Validation**
- **Graceful error handling for missing keys**:
```python
def validate_provider_config(provider: str, api_token: Optional[str] = None) -> str:
    """Validate and return API key for provider"""
    try:
        api_key = get_api_key(provider, api_token)
        return api_key
    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=f"Provider configuration error: {str(e)}"
        )
```

## **Request Model Patterns**
- **Flexible extraction configuration**:
```python
class ExtractionConfig(BaseModel):
    provider: str  # Supports all formats: "openai/gpt-4", "gpt-4.1-nano-2025-04-14"
    api_token: Optional[str] = None  # Optional - uses env vars if not provided
    instruction: str = "Extract the main content and key information"
    extra_args: Optional[Dict[str, Any]] = {}

class CrawlRequest(BaseModel):
    url: str
    extraction_strategy: Optional[str] = None
    extraction_config: Optional[ExtractionConfig] = None
```

## **Testing Patterns**
- **Test with multiple providers**:
```python
def test_multiple_providers():
    """Test extraction with different AI providers"""
    providers = [
        "gpt-4.1-nano-2025-04-14",
        "openai/gpt-4o-mini", 
        "anthropic/claude-3-sonnet",
        "google/gemini-pro"
    ]
    
    for provider in providers:
        config = ExtractionConfig(
            provider=provider,
            instruction="Extract key information"
        )
        # Test extraction...
```

## **Frontend Integration**
- **Provide user-friendly provider options**:
```javascript
const AI_PROVIDERS = [
    { value: "gpt-4.1-nano-2025-04-14", label: "GPT-4.1 Nano (Custom Model)" },
    { value: "openai/gpt-4o-mini", label: "OpenAI GPT-4o Mini" },
    { value: "anthropic/claude-3-sonnet", label: "Anthropic Claude 3 Sonnet" },
    { value: "google/gemini-pro", label: "Google Gemini Pro" }
];
```

## **Configuration Examples**
- **Common provider configurations**:
```python
# OpenAI with custom model
{
    "provider": "gpt-4.1-nano-2025-04-14",
    "instruction": "Extract main content"
}

# Prefixed format
{
    "provider": "anthropic/claude-3-sonnet",
    "api_token": "sk-ant-...",
    "instruction": "Summarize key points"
}

# With extra arguments
{
    "provider": "openai/gpt-4",
    "instruction": "Extract structured data",
    "extra_args": {
        "temperature": 0.1,
        "max_tokens": 1000
    }
}
```

## **File References**
- Main extraction logic: [rest_api.py](mdc:src/rest_api.py)
- Environment template: [.env.example](mdc:.env.example)
- API documentation: [POSTMAN_TESTING_GUIDE.md](mdc:POSTMAN_TESTING_GUIDE.md)
